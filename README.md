# Enhancing Consistent Visual Explanations through Saliency-Guided Contrastive Learning
Deep neural networks have revolutionized computer vision tasks with their exceptional performance. However, their lack of interpretability has raised concerns and sparked the need for effective explanation techniques. While various methods have been proposed, recent investigations have revealed biases that result in explanations inconsistent with human knowledge. Our solution: a breakthrough approach that tackles this challenge head-on. Rather than proposing a new method of interpretation, this study instructs models on how to learn unbiased features that lead to explanations that are more consistent with the task and humans' priors, given existing methods of interpretation.
![main image]

Code uploading in progress--
